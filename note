一、丢弃法
①丢弃法将一些输出项随机置0来控制模型复杂度
②常作用在多层感知机的隐藏层输出上
③丢弃概率是控制模型复杂度的超参数
这个方法与正则的效果差不多，但是目前的理论暂时不能解释。


二、数值稳定性
①梯度爆炸（可以用ReLU函数举例子）
1.值超出值域
2.对学习率敏感，学习率太大，导致更大的梯度，学习率太小，训练无进展。
②梯度消失（sigmoid）
1.梯度值变为0，训练没有进展
2.仅仅顶部层网络训练的较好，无法让神经网络更深。

如何让训练更加稳定。方法：
