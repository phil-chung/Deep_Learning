一、丢弃法
①丢弃法将一些输出项随机置0来控制模型复杂度
②常作用在多层感知机的隐藏层输出上
③丢弃概率是控制模型复杂度的超参数
这个方法与正则的效果差不多，但是目前的理论暂时不能解释。


二、数值稳定性
①梯度爆炸（可以用ReLU函数举例子）
1.值超出值域
2.对学习率敏感，学习率太大，导致更大的梯度，学习率太小，训练无进展。
②梯度消失（sigmoid）
1.梯度值变为0，训练没有进展
2.仅仅顶部层网络训练的较好，无法让神经网络更深。

如何让训练更加稳定。方法：

三、卷积
1.多通道输出卷积层
2.1*1卷积层

四、池化层
输出通道数=输入通道数
池化层返回窗口中最大或平均值
缓解卷积层对位置的敏感性

五、批量归一化
一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大；
另一方面，参数的变化导致每一层的输入数据分布会发生改变，进而上层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做Internal Covariate Shift。
（但后续有论文指出可能就是通过在每个小批量里加入噪音来控制模型复杂度）

六、微调
微调通过使用在大数据上得到的预训练好的模型初始化模型权重来完成提升精度。
